# ğŸ“– ProxyPool é¡¹ç›®æŒ‡å—

> ä¸€ä¸ªç®€æ˜“é«˜æ•ˆçš„ä»£ç†æ± ç³»ç»Ÿï¼Œæ”¯æŒè‡ªåŠ¨æŠ“å–ã€æµ‹è¯•ã€å­˜å‚¨å’Œ API æŸ¥è¯¢

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### æœ€å¿«ä¸Šæ‰‹ï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/498330580/ProxyPool.git
cd ProxyPool

# 2. Docker æ–¹å¼ï¼ˆæ¨èï¼‰
docker-compose up

# æˆ–å¸¸è§„æ–¹å¼
pip install -r requirements.txt
export PROXYPOOL_REDIS_HOST='localhost'
python run.py
```

è®¿é—®ï¼š`http://localhost:5555` è·å–éšæœºä»£ç† âœ…

---

## ğŸ“š é¡¹ç›®æ¦‚è¿°

### æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | æè¿° |
|------|------|
| ğŸ”„ **è‡ªåŠ¨çˆ¬å–** | ä»å¤šä¸ªå…è´¹ä»£ç†ç½‘ç«™å®šæ—¶è·å–ä»£ç† |
| âœ… **å¯ç”¨æ€§æ£€æµ‹** | å®šæœŸæµ‹è¯•ä»£ç†è¿é€šæ€§ï¼Œè‡ªåŠ¨å‰”é™¤ä¸å¯ç”¨ä»£ç† |
| ğŸ“¦ **Rediså­˜å‚¨** | ä»£ç†æŒ‰åˆ†æ•°æ’åºå­˜å‚¨ï¼Œæ”¯æŒå¤šå­æ± éš”ç¦» |
| ğŸš€ **API æ¥å£** | æä¾› `/random`, `/all`, `/count` ç­‰æ¥å£ |
| ğŸ”Œ **æ’ä»¶ç®¡ç†** | æ”¯æŒåŠ¨æ€åŠ è½½å…¬å…±/ç§æœ‰çˆ¬è™«æ’ä»¶ |

### ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Getter    â”‚       â”‚   Tester    â”‚       â”‚   Server    â”‚
â”‚  (çˆ¬å–ä»£ç†)  â”‚       â”‚  (æµ‹è¯•ä»£ç†)  â”‚       â”‚  (APIæ¥å£)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Redis     â”‚
                        â”‚  (ä»£ç†å‚¨å­˜)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

### Docker æ–¹å¼ï¼ˆæ¨èï¼‰
- ğŸ³ Docker
- ğŸ”„ Docker-Compose

### å¸¸è§„æ–¹å¼
- ğŸ Python >= 3.10
- ğŸ“¦ Redis >= 5.0

---

## ğŸ“‹ è¯¦ç»†é…ç½®

### Redis è¿æ¥

**æ–¹å¼ä¸€ï¼šåˆ†åˆ«è®¾ç½®ä¸»æœºã€ç«¯å£ã€å¯†ç **
```bash
export PROXYPOOL_REDIS_HOST='localhost'
export PROXYPOOL_REDIS_PORT=6379
export PROXYPOOL_REDIS_PASSWORD=''
export PROXYPOOL_REDIS_DB=0
```

**æ–¹å¼äºŒï¼šè¿æ¥å­—ç¬¦ä¸²ï¼ˆæ¨èï¼‰**
```bash
export PROXYPOOL_REDIS_CONNECTION_STRING='redis://[:password@]host[:port][/database]'
```

### å¸¸ç”¨ç¯å¢ƒå˜é‡

| å˜é‡ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `APP_ENV` | è¿è¡Œç¯å¢ƒ (dev/test/prod) | dev |
| `APP_DEBUG` | è°ƒè¯•æ¨¡å¼ | true |
| `API_HOST` | API ç›‘å¬åœ°å€ | 0.0.0.0 |
| `API_PORT` | API ç›‘å¬ç«¯å£ | 5555 |
| `CYCLE_GETTER` | è·å–å‘¨æœŸï¼ˆç§’ï¼‰ | 100 |
| `CYCLE_TESTER` | æµ‹è¯•å‘¨æœŸï¼ˆç§’ï¼‰ | 20 |
| `TEST_URL` | æµ‹è¯•ç›®æ ‡ URL | http://www.baidu.com |
| `TEST_TIMEOUT` | æµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | 10 |

---

## ğŸš€ å¯åŠ¨æ–¹å¼

### æ–¹å¼ä¸€ï¼šå…¨éƒ¨å¯åŠ¨ï¼ˆæ¨èï¼‰
```bash
python run.py
```
ä¼šè‡ªåŠ¨å¯åŠ¨ Getterã€Testerã€Server ä¸‰ä¸ªæ¨¡å—ã€‚

### æ–¹å¼äºŒï¼šæŒ‰éœ€å¯åŠ¨
```bash
python run.py --processor getter  # ä»…å¯åŠ¨çˆ¬å–æ¨¡å—
python run.py --processor tester  # ä»…å¯åŠ¨æµ‹è¯•æ¨¡å—
python run.py --processor server  # ä»…å¯åŠ¨ API æœåŠ¡
```

### Docker å¯åŠ¨
```bash
docker-compose up        # å‰å°è¿è¡Œ
docker-compose up -d     # åå°è¿è¡Œ
```

---

## ğŸ” API æ¥å£

### åŸºç¡€ä¿¡æ¯

- **åŸºç¡€ URL**ï¼š`http://{ä½ çš„æœåŠ¡å™¨IP}:5555`
- **é€šç”¨å‚æ•°**ï¼š`key`ï¼ˆå¯é€‰ï¼ŒæŒ‡å®šå­æ± ï¼Œå¦‚ `proxies:weibo`ï¼‰

### æ ¸å¿ƒæ¥å£

#### `GET /random` - è·å–éšæœºä»£ç†
```bash
curl http://localhost:5555/random
# è¿”å›ï¼š116.196.115.209:8080
```

#### `GET /all` - è·å–æ‰€æœ‰ä»£ç†
```bash
curl http://localhost:5555/all
# è¿”å›ï¼š(å¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ª host:port)
```

#### `GET /count` - è·å–ä»£ç†æ•°é‡
```bash
curl http://localhost:5555/count
# è¿”å›ï¼š123
```

### æŒ‡å®šå­æ± 
```bash
# è·å–å¾®åšä¸“ç”¨ä»£ç†
curl "http://localhost:5555/random?key=proxies:weibo"
```

### ç»Ÿè®¡æ¥å£

#### `GET /api/stats` - è·å–ç»Ÿè®¡ä¿¡æ¯
```json
{
  "proxy_count": 100,
  "crawler_count": 20,
  "status": "è¿è¡Œä¸­",
  "avg_score": 75
}
```

#### `GET /api/proxies` - åˆ†é¡µä»£ç†åˆ—è¡¨
```bash
curl "http://localhost:5555/api/proxies?limit=20&offset=0"
```

---

## ğŸŒ Web ç®¡ç†é¢æ¿

### è®¿é—®åœ°å€

| é¡µé¢ | é“¾æ¥ | åŠŸèƒ½ |
|------|------|------|
| é¦–é¡µ | `http://localhost:5555/` | ç³»ç»ŸçŠ¶æ€å±•ç¤º |
| **ä»ªè¡¨ç›˜** | `http://localhost:5555/admin` | å®æ—¶æ•°æ®ã€ä»£ç†åˆ—è¡¨ã€æœç´¢ã€åˆ†é¡µ |
| **æ’ä»¶ç®¡ç†** | `http://localhost:5555/admin/plugins` | æŸ¥çœ‹/åˆ›å»º/ä¸Šä¼ çˆ¬è™«æ’ä»¶ |
| **API æ–‡æ¡£** | `http://localhost:5555/admin/help` | API ä½¿ç”¨è¯´æ˜ã€ä»£ç ç¤ºä¾‹ |

### ä»ªè¡¨ç›˜åŠŸèƒ½

- ğŸ“Š **å®æ—¶ç»Ÿè®¡**ï¼šä»£ç†æ€»æ•°ã€çˆ¬è™«æ•°ã€ç³»ç»ŸçŠ¶æ€ã€å¹³å‡åˆ†æ•°
- ğŸ” **æœç´¢è¿‡æ»¤**ï¼šå¿«é€ŸæŸ¥æ‰¾æŒ‡å®šä»£ç†
- ğŸ“– **åˆ†é¡µæ˜¾ç¤º**ï¼šæ¯é¡µ 20 æ¡ï¼Œæ”¯æŒç¿»é¡µ
- ğŸ“‹ **å¤åˆ¶ä»£ç†**ï¼šä¸€é”®å¤åˆ¶åˆ°å‰ªè´´æ¿
- ğŸ“ **å¯¼å‡ºæ•°æ®**ï¼šæ”¯æŒ CSVã€JSONã€TXT æ ¼å¼
- ğŸ”„ **è‡ªåŠ¨åˆ·æ–°**ï¼š30 ç§’è‡ªåŠ¨æ›´æ–°æ•°æ®

### æ’ä»¶ç®¡ç†åŠŸèƒ½

- âœ¨ **æŸ¥çœ‹æ’ä»¶**ï¼šæ˜¾ç¤ºå·²åŠ è½½çš„å…¬å…±/ç§æœ‰çˆ¬è™«
- âœï¸ **åˆ›å»ºæ’ä»¶**ï¼šåœ¨çº¿ç¼–å†™ Python çˆ¬è™«ä»£ç 
- ğŸ“¤ **ä¸Šä¼ æ’ä»¶**ï¼šä¸Šä¼ æœ¬åœ°ç¼–å†™çš„ .py æ–‡ä»¶
- ğŸ”Œ **è‡ªåŠ¨åŠ è½½**ï¼šæ’ä»¶åˆ›å»ºåè‡ªåŠ¨è¢«ç³»ç»ŸåŠ è½½

---

## ğŸ”Œ çˆ¬è™«æ’ä»¶å¼€å‘

### å¿«é€Ÿåˆ›å»º

åœ¨æ’ä»¶ç®¡ç†é¡µé¢ç‚¹å‡»"åˆ›å»ºæ’ä»¶"æˆ–"ä¸Šä¼ æ’ä»¶"ï¼Œéµå¾ªè§„èŒƒç¼–å†™ä»£ç ã€‚

### ä»£ç è§„èŒƒ

```python
from pyquery import PyQuery as pq
from proxypool.schemas.proxy import Proxy
from proxypool.crawlers.base import BaseCrawler

BASE_URL = 'http://www.example.com/{page}.html'
MAX_PAGE = 5

class ExampleCrawler(BaseCrawler):
    """
    Example crawler, http://www.example.com
    """
    urls = [BASE_URL.format(page=page) for page in range(1, MAX_PAGE + 1)]

    def parse(self, html):
        """
        Parse HTML to extract proxies
        :return: yield Proxy objects
        """
        doc = pq(html)
        for item in doc('.proxy-item').items():
            host = item.find('.ip').text()
            port = int(item.find('.port').text())
            yield Proxy(host=host, port=port)
```

åœ¨è¿™é‡Œåªéœ€è¦å®šä¹‰ä¸€ä¸ª Crawler ç»§æ‰¿ BaseCrawler å³å¯ï¼Œç„¶åå®šä¹‰å¥½ urls å˜é‡å’Œ parse æ–¹æ³•å³å¯ã€‚

- urls å˜é‡å³ä¸ºçˆ¬å–çš„ä»£ç†ç½‘ç«™ç½‘å€åˆ—è¡¨ï¼Œå¯ä»¥ç”¨ç¨‹åºå®šä¹‰ä¹Ÿå¯å†™æˆå›ºå®šå†…å®¹ã€‚
- parse æ–¹æ³•æ¥æ”¶ä¸€ä¸ªå‚æ•°å³ htmlï¼Œä»£ç†ç½‘å€çš„ htmlï¼Œåœ¨ parse æ–¹æ³•é‡Œåªéœ€è¦å†™å¥½ html çš„è§£æï¼Œè§£æå‡º host å’Œ portï¼Œå¹¶æ„å»º Proxy å¯¹è±¡ yield è¿”å›å³å¯ã€‚

ç½‘é¡µçš„çˆ¬å–ä¸éœ€è¦å®ç°ï¼ŒBaseCrawler å·²ç»æœ‰äº†é»˜è®¤å®ç°ï¼Œå¦‚éœ€æ›´æ”¹çˆ¬å–æ–¹å¼ï¼Œé‡å†™ crawl æ–¹æ³•å³å¯ã€‚

### å…³é”®è¦ç‚¹

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| ğŸ“ **ä½ç½®** | ä¿å­˜åˆ° `proxypool/crawlers/private/` |
| ğŸ·ï¸ **å‘½å** | æ–‡ä»¶å `example.py`ï¼Œç±»å `ExampleCrawler` |
| ğŸ”— **ç»§æ‰¿** | å¿…é¡»ç»§æ‰¿ `BaseCrawler` |
| ğŸ“ **æ–¹æ³•** | å¿…é¡»å®ç° `parse(html)` æ–¹æ³• |
| ğŸ“¤ **è¿”å›** | ä½¿ç”¨ `yield Proxy(host=..., port=...)` |
| âœ… **å®¹é”™** | å¼‚å¸¸æ—¶è‡ªåŠ¨è·³è¿‡è¯¥æ’ä»¶ï¼Œä¸å½±å“å…¶ä»–æ’ä»¶ |

---

## ğŸ’» Python ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```python
import requests

def get_random_proxy():
    """è·å–éšæœºä»£ç†"""
    return requests.get('http://127.0.0.1:5555/random').text.strip()

def crawl_with_proxy(url, proxy):
    """ä½¿ç”¨ä»£ç†çˆ¬å–ç½‘é¡µ"""
    proxies = {'http': f'http://{proxy}'}
    return requests.get(url, proxies=proxies).text

# ä½¿ç”¨
proxy = get_random_proxy()
html = crawl_with_proxy('http://httpbin.org/get', proxy)
print(html)
```

### é«˜çº§ç”¨æ³•ï¼ˆå­æ± ï¼‰

```python
# åˆ›å»ºå¾®åšä¸“ç”¨ä»£ç†æ± 
import os
os.environ['REDIS_KEY'] = 'proxies:weibo'

# è·å–å¾®åšä¸“ç”¨ä»£ç†
proxy = requests.get('http://127.0.0.1:5555/random?key=proxies:weibo').text.strip()
```

---

## ğŸ“Š ç³»ç»Ÿç›‘æ§

### æŸ¥çœ‹æ—¥å¿—

```bash
tail -f logs/runtime.log    # è¿è¡Œæ—¥å¿—
tail -f logs/error.log      # é”™è¯¯æ—¥å¿—
```

### å…³é”®æŒ‡æ ‡

- é¦–å±åŠ è½½æ—¶é—´ï¼š< 1s
- æ•°æ®åˆ·æ–°é¢‘ç‡ï¼š30 ç§’
- æ”¯æŒæœ€å¤§ä»£ç†æ•°ï¼š50000+
- å•æ¬¡ API å“åº”æ—¶é—´ï¼š< 100ms

---

## âš™ï¸ é«˜çº§é…ç½®

### è‡ªå®šä¹‰ Redis å­æ± 

ä¸ºä¸åŒç›®æ ‡ç½‘ç«™åˆ›å»ºä¸“ç”¨ä»£ç†æ± ï¼š

```bash
# åˆ›å»ºå¾®åšä¸“ç”¨ä»£ç†æ± 
export REDIS_KEY='proxies:weibo'
export TEST_URL='http://weibo.cn'
python run.py

# å¦èµ·ä¸€ä¸ªç»ˆç«¯åˆ›å»ºè±†ç“£ä¸“ç”¨ä»£ç†æ± 
export REDIS_KEY='proxies:douban'
export TEST_URL='http://douban.com'
python run.py
```

### Docker å¤šæ± é…ç½®

ç¼–è¾‘ `docker-compose.yml`ï¼š

```yaml
services:
  proxypool:
    environment:
      PROXYPOOL_REDIS_HOST: redis
      REDIS_KEY: proxies:weibo
      TEST_URL: http://weibo.cn
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ï¼šæ— æ³•è·å–ä»£ç†

**æ£€æŸ¥æ¸…å•ï¼š**
- âœ… Redis æœåŠ¡æ˜¯å¦è¿è¡Œï¼š`redis-cli ping`
- âœ… Getter æ¨¡å—æ˜¯å¦å¯åŠ¨ï¼šæŸ¥çœ‹æ—¥å¿—
- âœ… ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼šè®¿é—®ä»£ç†æºç½‘ç«™
- âœ… é˜²ç«å¢™æ˜¯å¦å¼€æ”¾ 5555 ç«¯å£

### é—®é¢˜ï¼šä»£ç†å¯ç”¨æ€§ä½

**åŸå› ï¼š** ä½¿ç”¨çš„æ˜¯å…è´¹ä»£ç†ï¼Œè´¨é‡æœ‰é™ï¼Œä»…é€‚åˆå­¦ä¹ å’Œæµ‹è¯•

**è§£å†³æ–¹æ¡ˆï¼š**
1. å¢åŠ ä»£ç†æºï¼ˆç¼–å†™æ›´å¤šçˆ¬è™«ï¼‰
2. ä¼˜åŒ–æµ‹è¯•ç­–ç•¥ï¼ˆé™ä½è¶…æ—¶æ—¶é—´ï¼‰
3. è€ƒè™‘ä»˜è´¹ä»£ç†æœåŠ¡

### é—®é¢˜ï¼šæ’ä»¶åŠ è½½å¤±è´¥

**æ£€æŸ¥ï¼š**
- ä»£ç æ˜¯å¦ç»§æ‰¿ `BaseCrawler`
- æ˜¯å¦å®ç°äº† `parse()` æ–¹æ³•
- æ˜¯å¦ä½¿ç”¨ `yield Proxy()` è¿”å›ä»£ç†
- æ–‡ä»¶æ˜¯å¦ä¿å­˜åœ¨ `private` æ–‡ä»¶å¤¹

---

## ğŸ“– æ–‡æ¡£å¯¼èˆª

| æ–‡æ¡£ | å†…å®¹ |
|------|------|
| **README.md** | é¡¹ç›®è¯¦ç»†è¯´æ˜ã€æ‰€æœ‰ç¯å¢ƒå˜é‡é…ç½® |
| **OPTIMIZATION_SUMMARY.md** | Web æ§åˆ¶é¢æ¿ä¼˜åŒ–æ€»ç»“ |
| **PROJECT_GUIDE.md** | æœ¬æ–‡ä»¶ï¼Œå¿«é€Ÿå…¥é—¨æŒ‡å— |
| **doc/** | é¡¹ç›®æˆªå›¾å’Œå›¾æ–‡è¯´æ˜ |

---

## ğŸ”— å¸¸ç”¨é“¾æ¥

- ğŸ“¦ [Docker Hub å®˜æ–¹é•œåƒ](https://hub.docker.com/r/498330580/proxypool)
- ğŸ“š [åŸç†è®²è§£æ–‡ç« ](https://proxy-pool.yaoling.cc)
- ğŸ”— [GitHub ä»“åº“](https://github.com/498330580/ProxyPool)
- ğŸ’¬ [æäº¤ Issue](https://github.com/498330580/ProxyPool/issues)

---

## ğŸ“„ è®¸å¯è¯

MIT License

---

## ğŸ’¡ å°æç¤º

- ğŸŒŸ **æ¨èç”¨äºå­¦ä¹ **ï¼šè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ Python é¡¹ç›®å­¦ä¹ æ¡ˆä¾‹
- âš ï¸ **ä¸æ¨èç”¨äºç”Ÿäº§**ï¼šå…è´¹ä»£ç†è´¨é‡ä½ï¼Œå»ºè®®ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ä»˜è´¹ä»£ç†
- ğŸš€ **å®¹æ˜“æ‰©å±•**ï¼šé€šè¿‡æ·»åŠ çˆ¬è™«è½»æ¾æ‰©å±•ä»£ç†æº
- ğŸ”§ **çµæ´»é…ç½®**ï¼šæ‰€æœ‰å‚æ•°éƒ½å¯é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰

---

**æœ€åæ›´æ–°**ï¼š2025-11-09  
**æ›´æ–°å†…å®¹**ï¼šæ·»åŠ æ’ä»¶ä¸Šä¼ åŠŸèƒ½ã€çˆ¬è™«å®¹é”™æœºåˆ¶ã€Web ç®¡ç†é¢æ¿ä¼˜åŒ–
